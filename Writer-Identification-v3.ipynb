{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f18843a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0947775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\AppData\\Local\\Temp\\ipykernel_19964\\774798532.py:12: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "159a41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your handwriting dataset\n",
    "dataset_dir = 'Resized-Datasets'\n",
    "\n",
    "# Load the dataset\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Assuming each subdirectory in the dataset directory corresponds to a different writer\n",
    "for writer_dir in os.listdir(dataset_dir):\n",
    "    writer_images = []\n",
    "    writer_labels = []\n",
    "    writer_path = os.path.join(dataset_dir, writer_dir)\n",
    "#     print(\"writer_path: \", writer_path)\n",
    "    \n",
    "    # Assuming each image file in the writer directory corresponds to a handwriting sample\n",
    "    for image_file in os.listdir(writer_path):\n",
    "        image_path = os.path.join(writer_path, image_file)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "#         img = cv2.resize(img, (28, 28))\n",
    "        writer_images.append(img)\n",
    "        writer_labels.append(int(writer_dir))  # Assuming the writer directory is named with a numerical label\n",
    "#         writer_labels.append(writer_dir)\n",
    "    \n",
    "    # images += writer_images\n",
    "    images.extend(writer_images)\n",
    "    labels.extend(writer_labels)\n",
    "\n",
    "# Load the handwriting data (replace with your own data loading code)\n",
    "# Convert the lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b12684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the images\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Determine the number of unique labels in your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3b2b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_directory\\writer_identification\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 53ms/step - loss: 0.5622 - accuracy: 0.7208 - val_loss: 1.0577 - val_accuracy: 0.4778\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.3861 - accuracy: 0.8236 - val_loss: 1.0184 - val_accuracy: 0.4778\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.3004 - accuracy: 0.8681 - val_loss: 0.6581 - val_accuracy: 0.5111\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.2698 - accuracy: 0.8847 - val_loss: 0.8049 - val_accuracy: 0.4778\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.2159 - accuracy: 0.9208 - val_loss: 0.7839 - val_accuracy: 0.4778\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.1722 - accuracy: 0.9306 - val_loss: 0.7463 - val_accuracy: 0.4778\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.1482 - accuracy: 0.9403 - val_loss: 0.7660 - val_accuracy: 0.4778\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.1234 - accuracy: 0.9528 - val_loss: 0.6028 - val_accuracy: 0.5889\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.0935 - accuracy: 0.9681 - val_loss: 0.5781 - val_accuracy: 0.6056\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0845 - accuracy: 0.9681 - val_loss: 0.5399 - val_accuracy: 0.8944\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5399 - accuracy: 0.8944\n",
      "Loss: 0.5398641228675842\n",
      "Test accuracy: 0.894444465637207\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Convolutional layer\n",
    "    model.add(keras.layers.Conv2D(\n",
    "        filters=hp.Int('conv_filters', min_value=16, max_value=128, step=16),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1)\n",
    "    ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "#     # Additional cConvolutional layer\n",
    "#     model.add(keras.layers.Conv2D(\n",
    "#         filters=hp.Int('conv_filters_2', min_value=16, max_value=128, step=16),\n",
    "#         kernel_size=(3, 3),\n",
    "#         activation='relu',\n",
    "#         input_shape=(28, 28, 1)\n",
    "#     ))\n",
    "#     model.add(keras.layers.BatchNormalization())\n",
    "#     model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Flatten the output from the convolutional layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    # Dense hidden layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=256, step=64),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    # Dense output layer\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='tuner_directory',\n",
    "    project_name='writer_identification'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the trained model\n",
    "best_model.save('writerIdentifier_v3_model.h5')\n",
    "\n",
    "# Evaluate the best model\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbee27d",
   "metadata": {},
   "source": [
    "convolayer: max=128, trial == 10  \n",
    "Loss: 0.5398641228675842\n",
    "Test accuracy: 0.894444465637207\n",
    "\n",
    "convolayer: max=128, trial == 15  \n",
    "Loss: 0.548403263092041\n",
    "Test accuracy: 0.7833333611488342"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b2a88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'conv_filters': 64, 'dense_units': 128}\n"
     ]
    }
   ],
   "source": [
    "print('Best Hyperparameters:')\n",
    "print(best_hp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d931fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def resizeImage(image_number): \n",
    "\n",
    "    input_dir = 'Prediction-Test-Datasets'\n",
    "\n",
    "    output_dir = 'Resized-Prediction-Test-Datasets'\n",
    "    target_size = (28, 28)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load the image\n",
    "    img_path = f\"{input_dir}/Predict-{image_number}.jpg\"\n",
    "    img = Image.open(img_path)\n",
    "        \n",
    "    # Resize the image while maintaining the aspect ratio using thumbnail method\n",
    "    img.thumbnail(target_size, Image.ANTIALIAS)\n",
    "        \n",
    "    # Create a new image with the target size as canvas\n",
    "    resized_image = Image.new('L', target_size, 255)\n",
    "        \n",
    "    # Paste the resized image onto the canvas\n",
    "    offset = ((target_size[0] - img.size[0]) // 2, (target_size[1] - img.size[1]) // 2)\n",
    "    resized_image.paste(img, offset)\n",
    "        \n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb1c9959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Writer: Janice\n",
      "Confidence: 0.53580874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\AppData\\Local\\Temp\\ipykernel_19964\\4217639828.py:19: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img.thumbnail(target_size, Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('writerIdentifier_model.h5')\n",
    "\n",
    "# Preprocess the input image\n",
    "image_number = 1\n",
    "resized_image = resizeImage(image_number)\n",
    "input_image = np.array(resized_image)\n",
    "# Normalise\n",
    "input_image = input_image.reshape(1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Convert the input image to a TensorFlow tensor\n",
    "input_tensor = tf.convert_to_tensor(input_image)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model(input_tensor)\n",
    "\n",
    "# Convert the predictions to a NumPy array\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "\n",
    "# Get the writer with the highest probability\n",
    "predicted_writer_index = np.argmax(predictions)\n",
    "confidence = np.max(predictions)\n",
    "\n",
    "# Map the predicted index to the actual writer label\n",
    "# 1 - Janice; 2 - Jasmine\n",
    "writers = [1, 2]  # List of writer labels used during training\n",
    "predicted_writer = writers[predicted_writer_index]\n",
    "\n",
    "# Print the predicted writer and confidence score\n",
    "print('Predicted Writer:', \"Janice\" if predicted_writer == 1 else \"Jasmine\")\n",
    "print('Confidence:', confidence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
